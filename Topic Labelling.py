# -*- coding: utf-8 -*-
"""Topic Labelling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aqk4E6VSc89VOoyICmgBzex50i-rCLHa

# Install Packages

####imports
"""

# Commented out IPython magic to ensure Python compatibility.
# import commonly used libraries
import pandas as pd
import numpy as np
from datetime import datetime
import seaborn as sns

import matplotlib.pyplot as plt
# %matplotlib inline

##NLP
from pyspark import SparkContext, SparkConf
from pyspark.sql import SQLContext
import sparknlp
from sparknlp.base import *
from sparknlp.annotator import *
from sparknlp.pretrained import PretrainedPipeline
from pyspark.sql import functions as F
from pyspark.ml import Pipeline
from sparknlp.base import DocumentAssembler, Finisher
from sparknlp.annotator import Tokenizer, Normalizer, LemmatizerModel, PerceptronModel,Chunker
from pyspark.ml.feature import RegexTokenizer, StopWordsRemover

"""#load Data"""

# Start SparkSession with Spark NLP
spark = sparknlp.start(gpu=True)
sqlContext = SQLContext(spark)

print("Spark NLP version: {}".format(sparknlp.version()))
print("Apache Spark version: {}".format(spark.version))

df = spark.read.format("csv").option("header", "true").option("inferSchema", "true").option("multiLine", "true").option('escape', '"').load('gs://bucket_name/the-reddit-covid-dataset-comments.csv')

"""####clean up"""

df.show(5)

"""#Pipeline"""

from sparknlp.pretrained import ResourceDownloader

#point to text col and drop nulls
text_col = 'body'
df = df.withColumn('content',F.col('body'))

text_col = 'body'
review_text = df.select(text_col).filter(F.col(text_col).isNotNull())

documentAssembler = DocumentAssembler().setInputCol(text_col).setOutputCol('document')

tokenizer = Tokenizer().setInputCols(['document']).setOutputCol('tokenized')

normalizer = Normalizer().setInputCols(['tokenized']).setOutputCol('normalized').setLowercase(True)

lemmatizer = LemmatizerModel.pretrained().setInputCols(['normalized']).setOutputCol('lemmatized')

stopword_en = nltk.corpus.stopwords.words('english')
stopword_es = nltk.corpus.stopwords.words('spanish')
stopword_fr = nltk.corpus.stopwords.words('french')
stopword_gr = nltk.corpus.stopwords.words('german')
stopwords = stopword_en + stopword_es + stopword_fr + stopword_gr

stopwords_cleaner = StopWordsCleaner().setInputCols(['lemmatized']).setOutputCol('no_stop_lemmatized').setStopWords(stopwords)

pos_tagger = PerceptronModel.pretrained('pos_anc').setInputCols(['document', 'lemmatized']).setOutputCol('pos')

allowed_tags = ['<JJ>+<NN>', '<NN>+<NN>']
chunker = Chunker().setInputCols(['document', 'pos']).setOutputCol('ngrams').setRegexParsers(allowed_tags)

finisher = Finisher().setInputCols(['ngrams','no_stop_lemmatized'])

pipeline = Pipeline().setStages([documentAssembler,
                 tokenizer,
                 normalizer,
                 lemmatizer,
                 stopwords_cleaner,
                 pos_tagger,
                 chunker,
                 finisher])

processed_review = pipeline.fit(review_text).transform(review_text)

processed_review.show(5)

from pyspark.sql.functions import concat
processed_review = processed_review.withColumn('final',
     concat(F.col('finished_ngrams'), 
            F.col('finished_no_stop_lemmatized')))

from pyspark.ml.feature import CountVectorizer
tfizer = CountVectorizer(inputCol='finished_no_stop_lemmatized',
                         outputCol='tf_features')
tf_model = tfizer.fit(processed_review)
tf_result = tf_model.transform(processed_review)

from pyspark.ml.feature import IDF
idfizer = IDF(inputCol='tf_features', 
              outputCol='tf_idf_features')
idf_model = idfizer.fit(tf_result)
tfidf_result = idf_model.transform(tf_result)

from pyspark.ml.clustering import LDA

num_topics = 10
max_iter = 10
lda = LDA(k=num_topics, 
          maxIter=max_iter, 
          featuresCol='tf_idf_features')
lda_model = lda.fit(tfidf_result)

vocab = tf_model.vocabulary
def get_words(token_list):
    return [vocab[token_id] for token_id in token_list]
udf_to_words = F.udf(get_words)

num_top_words = 7
topics = lda_model.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))

topics.select('topic', 'topicWords').show(truncate=100)

