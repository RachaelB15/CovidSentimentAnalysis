# -*- coding: utf-8 -*-
"""notebooks_jupyter_Sentiment_on_RedditData_RB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qL4d6T11IffGBj8NPX_1tJVQWtjXtxCY

#### imports
"""

import io, requests
import sparknlp
from sparknlp.pretrained import PretrainedPipeline 
from sparknlp.annotator import *
from sparknlp.base import *

"""## initiate and load data"""

import sparknlp
spark = sparknlp.start(gpu=True)

print("Spark NLP version: {}".format(sparknlp.version()))
print("Apache Spark version: {}".format(spark.version))

"""#### load from GCP"""

df = spark.read.format("csv").option("header", "true").option("inferSchema", "true").option("multiLine", "true").option('escape', '"').load('gs://bucket_name/the-reddit-covid-dataset-comments.csv')

df.show(20)

"""# Pipelines

## GloVe
"""

pipeline = PretrainedPipeline('analyze_sentimentdl_glove_imdb', 'en')

sent_result = pipeline.annotate(df,column='body')

sent_result.select('sentiment').show(10,truncate=False)

sent_result=sent_result.withColumn('sentiment.result',sent_result.sentiment.getItem('result'))
sent_result.show(5)

sent_sum=sent_result.select('`subreddit.name`','sentiment.result')

#sent_sum.select('result').groupby('result').count().show()

#sent_sum.groupby('subreddit.name').pivot('result').show()

"""## language detection"""

pipeline = PretrainedPipeline("detect_language_43", lang = "xx")

lang_result= pipeline.annotate(df, column='body')

lang_result.show(2)

lang_result=lang_result.withColumn('language result',lang_result.language.getItem('result'))
lang_result.show(5)

lang_result_mini=lang_result.limit(100000)

res= lang_result_mini.select('language result').groupby('language result').count()

res2=lang_result.groupby('`subreddit.name`').pivot('language result')

#res.show(40)

#res2.show(5)

"""### Please check JSL labs @ [Models Hub](https://nlp.johnsnowlabs.com/models) for more pretrained models and pipelines! ðŸ˜Š """
